{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f58184-cabb-4023-933d-6cf71b6909d7",
   "metadata": {},
   "source": [
    "# Notebook to scrape Transfermarket\n",
    "\n",
    "## You have to manually input the data for the league you want to scrape, try mixing it up and do some lower leagues as well. \n",
    "\n",
    "## When saving to a csv, indicate what tier the league you scraped for was in, as well as what the league is, so we can put that data in.\n",
    "\n",
    "# Remember to change the csv, when scraping a new league so you do not overwrite the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a57a766-7fe6-4738-a678-79f8b2dd164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "base_url = f'https://www.transfermarkt.com'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def extract_and_strip_text(element):\n",
    "    if isinstance(element, str):\n",
    "        return element.strip()\n",
    "    elif element: \n",
    "        return element.text.strip()\n",
    "    return \"N/A\" \n",
    "\n",
    "\n",
    "def get_info(labels, results):\n",
    "    if not results:\n",
    "        return None\n",
    "    if isinstance(labels, str):\n",
    "        labels = [labels]\n",
    "    for label in labels:\n",
    "        element = results.find('span', class_='info-table__content', string=label)\n",
    "        if element:\n",
    "            value = element.find_next_sibling('span', class_='info-table__content--bold')\n",
    "            if value:\n",
    "                return value.get_text(strip=True)\n",
    "    return None\n",
    "\n",
    "def get_info_youth(labels, results):\n",
    "    if not results:\n",
    "        return None\n",
    "    if isinstance(labels, str):\n",
    "        labels = [labels]\n",
    "    for label in labels:\n",
    "        element = results.find('h2', class_='content-box-headline', string=label)\n",
    "        if element:\n",
    "            value = element.find_next_sibling('div', class_='content')\n",
    "            if value:\n",
    "                return value.get_text(strip=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_transfer_details(data):\n",
    "    transfers = []\n",
    "    for transfer in data:\n",
    "        transfer_details = {\n",
    "            'From Club': transfer['from']['clubName'],\n",
    "            'To Club': transfer['to']['clubName'],\n",
    "            'Date of Transfer': transfer['date'],\n",
    "            'Unformatted Date': transfer['dateUnformatted'],\n",
    "            'Fee': transfer['fee'],\n",
    "            'Season': transfer['season'],\n",
    "            'Market Value': transfer['marketValue']\n",
    "        }\n",
    "        transfers.append(transfer_details)\n",
    "    return transfers\n",
    "\n",
    "\n",
    "seen_urls = set()\n",
    "all_transfer_data = []\n",
    "\n",
    "\n",
    "for num in range(23, 22, -1):  # Adjust to your actual range The range right now is only for 1 year.\n",
    "    URL = f\"https://www.transfermarkt.com/premier-league/startseite/wettbewerb/GB1/plus/?saison_id=20{num}\"\n",
    "\n",
    "    rand = random.randint(10, 15) \n",
    "    time.sleep(rand)\n",
    "    \n",
    "    season_page = requests.get(URL, headers=headers)\n",
    "    each_season_soup = BeautifulSoup(season_page.content, \"html.parser\")\n",
    "\n",
    "    season_results = each_season_soup.find(\"table\", class_=\"items\")\n",
    "    season_listResults1 = season_results.find(\"tbody\")\n",
    "    season_listResults = season_listResults1.find_all(\"tr\")\n",
    "\n",
    "    for season_element in season_listResults:\n",
    "        if season_element.find(\"td\", class_=\"extrarow\"):\n",
    "            continue\n",
    "        season_name_element = season_element.find(\"td\").find(\"a\")[\"href\"]\n",
    "\n",
    "        if season_name_element in seen_urls:\n",
    "            continue\n",
    "        else:\n",
    "            seen_urls.add(season_name_element)\n",
    "\n",
    "        team_url = f'https://www.transfermarkt.com{season_name_element}'\n",
    "\n",
    "        rand = random.randint(10, 15) \n",
    "        time.sleep(rand)\n",
    "    \n",
    "        team_page = requests.get(team_url, headers=headers)\n",
    "        team_soup = BeautifulSoup(team_page.content, \"html.parser\")\n",
    "\n",
    "        team_results = team_soup.find(\"table\", class_=\"items\")\n",
    "        team_listResults1 = team_results.find(\"tbody\")\n",
    "        team_listResults2 = team_listResults1.find_all(\"td\", class_=\"posrela\")\n",
    "\n",
    "        team_listResults = [team_element.find_all(\"td\", class_=\"hauptlink\") for team_element in team_listResults2]\n",
    "\n",
    "        for team_element in team_listResults:\n",
    "            for team_cell in team_element:\n",
    "                team_name_element = team_cell.a.text.strip()\n",
    "                team_link = team_cell.a['href'] if team_cell.a else None\n",
    "\n",
    "                player_link = f'https://www.transfermarkt.com{team_link}'\n",
    "\n",
    "                name_in_link = player_link.split(\"transfermarkt.com/\")[1].split(\"/profil\")[0]\n",
    "                \n",
    "                rand = random.randint(10, 15) \n",
    "                time.sleep(rand)\n",
    "    \n",
    "                player_page = requests.get(player_link, headers=headers)\n",
    "                soup = BeautifulSoup(player_page.content, \"html.parser\")\n",
    "\n",
    "                results1 = soup.find(\"div\", class_=\"box tm-player-additional-data\")\n",
    "                if results1:\n",
    "                    youth_club_info = results1.find(\"div\", class_=\"content\").text.strip() if results1.find(\"div\", class_=\"content\") else \"Empty\"\n",
    "                else:\n",
    "                    youth_club_info = \"Empty\"\n",
    "                results = soup.find(\"div\", class_=\"info-table\")\n",
    "\n",
    "                # Extract key data\n",
    "                player_data = {\n",
    "                    'Full name': get_info(['Full name:', 'Name in home country:'], results),\n",
    "                    'Name Alternative' : name_in_link,\n",
    "                    'Date of birth/Age': get_info('Date of birth/Age:', results),\n",
    "                    'Place of birth': get_info('Place of birth:', results),\n",
    "                    'Height': get_info('Height:', results),\n",
    "                    'Citizenship': get_info('Citizenship:', results),\n",
    "                    'Position': get_info('Position:', results),\n",
    "                    'Foot': get_info('Foot:', results),\n",
    "                    'Player agent': get_info('Player agent:', results),\n",
    "                    'Current club': get_info('Current club:', results),\n",
    "                    'Joined': get_info('Joined:', results),\n",
    "                    'Contract expires': get_info('Contract expires:', results),\n",
    "                    'Last contract extension': get_info('Last contract extension:', results),\n",
    "                    'Outfitter': get_info('Outfitter:', results),\n",
    "                    'Youth Club': get_info_youth('Youth clubs', results1)\n",
    "                }\n",
    "\n",
    "\n",
    "                player_id = player_link.split(\"/\")[-1]\n",
    "                \n",
    "                transfer_url = f'https://www.transfermarkt.com/ceapi/transferHistory/list/{player_id}'\n",
    "\n",
    "                rand = random.randint(10, 15) #A random time and sleep was used to ensure the site is not flooded with requests.\n",
    "                time.sleep(rand)\n",
    "    \n",
    "                response = requests.get(transfer_url, headers=headers)\n",
    "                leagues = response.json().get('transfers', [])\n",
    "                transfers = extract_transfer_details(leagues)\n",
    "\n",
    "                # for transfer in transfers:\n",
    "                #     # Combine player data with transfer data\n",
    "                #     player_data.update(transfer)\n",
    "                #     all_transfer_data.append(player_data)\n",
    "\n",
    "                for transfer in transfers:\n",
    "                    combined_data = player_data.copy()\n",
    "                    combined_data.update(transfer)\n",
    "                    all_transfer_data.append(combined_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_name = 'Premier_League_Transfermarkt_scrape.csv'\n",
    "write_header = not os.path.exists(file_name)\n",
    "\n",
    "with open(file_name, mode='a', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=all_transfer_data[0].keys())\n",
    "    if write_header:\n",
    "        writer.writeheader() \n",
    "    writer.writerows(all_transfer_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
